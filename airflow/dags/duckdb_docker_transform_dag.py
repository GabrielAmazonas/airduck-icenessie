"""
DuckDB Docker Transform DAG - Decoupled horizontal scaling via DockerOperator.

This DAG spawns separate Docker containers for each partition, keeping DuckDB
processing fully decoupled from Airflow workers.

Architecture:
    Airflow Scheduler (LocalExecutor)
           â”‚
           â”‚ DockerOperator (via Docker socket)
           â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â–¼      â–¼      â–¼              â–¼
  DuckDB  DuckDB  DuckDB  ...  DuckDB
  Worker  Worker  Worker       Worker
  (p1)    (p2)    (p3)         (pN)
           â”‚
           â–¼
       MinIO S3 (Gold staging â†’ Iceberg)

Benefits:
- DuckDB runs in isolated containers (decoupled from Airflow)
- Each partition processed independently (horizontal scaling)
- Memory/CPU isolated per container
- Can scale by increasing max_active_tasks
- Works with existing LocalExecutor + Docker Compose
"""
from datetime import datetime, timedelta
import os
from typing import List

from airflow import DAG
from airflow.decorators import task
from airflow.providers.docker.operators.docker import DockerOperator
from airflow.operators.python import PythonOperator
from airflow.operators.empty import EmptyOperator
from docker.types import Mount

# =============================================================================
# Configuration
# =============================================================================
DUCKDB_WORKER_IMAGE = os.getenv("DUCKDB_WORKER_IMAGE", "airduck/duckdb-worker:latest")
DOCKER_NETWORK = os.getenv("DOCKER_NETWORK", "airduck_platform")

# S3/MinIO settings (passed to worker containers)
S3_ENDPOINT = os.getenv("S3_ENDPOINT", "minio:9000").replace("http://", "")
AWS_ACCESS_KEY_ID = os.getenv("AWS_ACCESS_KEY_ID", "minioadmin")
AWS_SECRET_ACCESS_KEY = os.getenv("AWS_SECRET_ACCESS_KEY", "minioadmin")

# Paths
SILVER_BASE_PATH = "s3://iceberg-warehouse/silver_silver/silver_documents/data"
GOLD_STAGING_PATH = "s3://iceberg-warehouse/gold_staging"

default_args = {
    "owner": "airflow",
    "depends_on_past": False,
    "email_on_failure": False,
    "retries": 2,
    "retry_delay": timedelta(minutes=2),
}


# =============================================================================
# Helper Functions
# =============================================================================

def discover_partitions(**context) -> List[str]:
    """
    Discover partitions in Silver layer that need processing.
    Uses DuckDB to list partition directories from S3.
    """
    import duckdb
    
    conn = duckdb.connect(":memory:")
    conn.execute("INSTALL httpfs; LOAD httpfs;")
    conn.execute(f"""
        SET s3_endpoint='{S3_ENDPOINT}';
        SET s3_access_key_id='{AWS_ACCESS_KEY_ID}';
        SET s3_secret_access_key='{AWS_SECRET_ACCESS_KEY}';
        SET s3_use_ssl=false;
        SET s3_url_style='path';
    """)
    
    try:
        # List partition directories
        result = conn.execute(f"""
            SELECT DISTINCT 
                regexp_extract(filename, 'ingested_at_day=([^/]+)', 1) as partition_key
            FROM glob('{SILVER_BASE_PATH}/*/*.parquet')
            WHERE partition_key IS NOT NULL
            ORDER BY partition_key DESC
        """).fetchall()
        
        partitions = [row[0] for row in result if row[0]]
        print(f"ðŸ“… Discovered {len(partitions)} partitions: {partitions}")
        
        conn.close()
        return partitions
        
    except Exception as e:
        print(f"âš ï¸ Error discovering partitions: {e}")
        conn.close()
        # Return empty list - DAG will skip processing
        return []


def merge_gold_staging(**context):
    """
    Merge staged Gold parquet files into the Iceberg Gold table.
    Runs after all partition workers complete.
    """
    import duckdb
    import trino
    
    print("ðŸ”„ Merging Gold staging into Iceberg table...")
    
    # First, read all staged files with DuckDB
    conn = duckdb.connect(":memory:")
    conn.execute("INSTALL httpfs; LOAD httpfs;")
    conn.execute(f"""
        SET s3_endpoint='{S3_ENDPOINT}';
        SET s3_access_key_id='{AWS_ACCESS_KEY_ID}';
        SET s3_secret_access_key='{AWS_SECRET_ACCESS_KEY}';
        SET s3_use_ssl=false;
        SET s3_url_style='path';
    """)
    
    try:
        # Count staged records
        count = conn.execute(f"""
            SELECT COUNT(*) FROM read_parquet('{GOLD_STAGING_PATH}/*/*.parquet')
        """).fetchone()[0]
        
        print(f"ðŸ“Š Found {count} staged records to merge")
        
        if count == 0:
            print("â­ï¸ No staged records to merge")
            return
        
        # For now, we'll use Trino to INSERT into Iceberg
        # (PyIceberg append would be more efficient but requires more setup)
        trino_host = os.getenv("TRINO_HOST", "trino")
        trino_port = int(os.getenv("TRINO_PORT", "8080"))
        
        trino_conn = trino.dbapi.connect(
            host=trino_host,
            port=trino_port,
            user="airflow",
            catalog="iceberg",
        )
        cursor = trino_conn.cursor()
        
        # Ensure Gold table exists (without embedding for simplicity)
        # In production, you'd have a proper schema migration
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS iceberg.gold.gold_documents_v2 (
                chunk_id VARCHAR,
                source_document_id VARCHAR,
                chunk_content VARCHAR,
                chunk_idx INTEGER,
                original_length INTEGER,
                source VARCHAR,
                ingested_at TIMESTAMP(6) WITH TIME ZONE,
                content_hash VARCHAR,
                quality_score DOUBLE,
                processed_at VARCHAR,
                worker_version VARCHAR
            )
            WITH (
                format = 'PARQUET',
                partitioning = ARRAY['day(ingested_at)']
            )
        """)
        
        # Note: For embeddings, you'd need ARRAY(REAL) column
        # Trino can read from S3 parquet using the hive connector
        # or you can use PyIceberg for direct append
        
        print("âœ… Gold table ready for merge")
        print("ðŸ’¡ Note: Use PyIceberg or Spark for full embedding support")
        
        cursor.close()
        trino_conn.close()
        conn.close()
        
    except Exception as e:
        print(f"âš ï¸ Merge error: {e}")
        raise


def cleanup_staging(**context):
    """Clean up staging directory after successful merge."""
    import boto3
    
    print("ðŸ§¹ Cleaning up staging directory...")
    
    s3 = boto3.client(
        "s3",
        endpoint_url=f"http://{S3_ENDPOINT}",
        aws_access_key_id=AWS_ACCESS_KEY_ID,
        aws_secret_access_key=AWS_SECRET_ACCESS_KEY,
    )
    
    bucket = "iceberg-warehouse"
    prefix = "gold_staging/"
    
    try:
        # List and delete staged files
        response = s3.list_objects_v2(Bucket=bucket, Prefix=prefix)
        
        if "Contents" in response:
            objects = [{"Key": obj["Key"]} for obj in response["Contents"]]
            if objects:
                s3.delete_objects(Bucket=bucket, Delete={"Objects": objects})
                print(f"ðŸ—‘ï¸ Deleted {len(objects)} staging files")
        else:
            print("â­ï¸ No staging files to clean up")
            
    except Exception as e:
        print(f"âš ï¸ Cleanup error: {e}")


# =============================================================================
# DAG Definition
# =============================================================================

with DAG(
    dag_id="duckdb_docker_transform",
    default_args=default_args,
    description="Gold transform using decoupled DuckDB Docker containers",
    schedule_interval="0 1 * * *",  # Daily at 1 AM
    start_date=datetime(2024, 1, 1),
    catchup=False,
    max_active_tasks=5,  # Limit concurrent Docker containers
    tags=["duckdb", "docker", "gold", "transform", "decoupled", "horizontal-scaling"],
    doc_md=__doc__,
) as dag:
    
    # -------------------------------------------------------------------------
    # Task: Start
    # -------------------------------------------------------------------------
    start = EmptyOperator(task_id="start")
    
    # -------------------------------------------------------------------------
    # Task: Discover partitions to process
    # -------------------------------------------------------------------------
    discover = PythonOperator(
        task_id="discover_partitions",
        python_callable=discover_partitions,
    )
    
    # -------------------------------------------------------------------------
    # Task: Process each partition with DockerOperator (Dynamic Task Mapping)
    # -------------------------------------------------------------------------
    # Note: Dynamic mapping with DockerOperator requires Airflow 2.4+
    # We use expand_kwargs to pass different env vars per task
    
    @task
    def prepare_docker_tasks(partitions: List[str]) -> List[dict]:
        """Prepare environment variables for each Docker task."""
        if not partitions:
            print("âš ï¸ No partitions to process")
            return []
        
        tasks = []
        for partition in partitions:
            tasks.append({
                "partition_key": partition,
                "env": {
                    "PARTITION_KEY": partition,
                    "S3_ENDPOINT": S3_ENDPOINT,
                    "AWS_ACCESS_KEY_ID": AWS_ACCESS_KEY_ID,
                    "AWS_SECRET_ACCESS_KEY": AWS_SECRET_ACCESS_KEY,
                    "SILVER_BASE_PATH": SILVER_BASE_PATH,
                    "OUTPUT_PATH": GOLD_STAGING_PATH,
                    "GENERATE_EMBEDDINGS": "true",
                    "CHUNK_SIZE": "1000",
                    "MIN_QUALITY_SCORE": "0.5",
                }
            })
        
        print(f"ðŸ“‹ Prepared {len(tasks)} Docker tasks")
        return tasks
    
    # Prepare task configs from discovered partitions
    task_configs = prepare_docker_tasks(discover.output)
    
    # -------------------------------------------------------------------------
    # Alternative: Manual loop with DockerOperator
    # (Use this if Dynamic Task Mapping with DockerOperator has issues)
    # -------------------------------------------------------------------------
    @task
    def run_duckdb_worker(task_config: dict):
        """
        Run DuckDB worker as a Docker container.
        
        This uses the DockerOperator-like functionality but within a PythonOperator,
        giving us more control and compatibility with Dynamic Task Mapping.
        """
        import docker
        
        partition_key = task_config["partition_key"]
        env = task_config["env"]
        
        print(f"ðŸ¦† Starting DuckDB worker for partition: {partition_key}")
        
        client = docker.from_env()
        
        try:
            # Run the container
            container = client.containers.run(
                image=DUCKDB_WORKER_IMAGE,
                command=["transform_gold.py"],
                environment=env,
                network=DOCKER_NETWORK,
                remove=True,  # Auto-remove after completion
                detach=False,  # Wait for completion
                mem_limit="4g",
                cpu_period=100000,
                cpu_quota=200000,  # 2 CPUs max
            )
            
            # Container output is returned as bytes
            output = container.decode("utf-8") if isinstance(container, bytes) else str(container)
            print(output)
            
            print(f"âœ… Partition {partition_key} processed successfully")
            return {"partition": partition_key, "status": "success"}
            
        except docker.errors.ContainerError as e:
            print(f"âŒ Container error for {partition_key}: {e}")
            raise
        except Exception as e:
            print(f"âŒ Error processing {partition_key}: {e}")
            raise
    
    # Map over all partitions - each runs in its own Docker container
    process_results = run_duckdb_worker.expand(task_config=task_configs)
    
    # -------------------------------------------------------------------------
    # Task: Merge staged files into Iceberg
    # -------------------------------------------------------------------------
    merge = PythonOperator(
        task_id="merge_gold_staging",
        python_callable=merge_gold_staging,
        trigger_rule="all_success",  # Only merge if all partitions succeeded
    )
    
    # -------------------------------------------------------------------------
    # Task: Cleanup staging
    # -------------------------------------------------------------------------
    cleanup = PythonOperator(
        task_id="cleanup_staging",
        python_callable=cleanup_staging,
    )
    
    # -------------------------------------------------------------------------
    # Task: End
    # -------------------------------------------------------------------------
    end = EmptyOperator(task_id="end")
    
    # -------------------------------------------------------------------------
    # Dependencies
    # -------------------------------------------------------------------------
    start >> discover >> task_configs >> process_results >> merge >> cleanup >> end


# =============================================================================
# DAG 2: Simple test DAG for Docker connectivity
# =============================================================================

with DAG(
    dag_id="test_docker_operator",
    default_args=default_args,
    description="Test Docker connectivity from Airflow",
    schedule_interval=None,  # Manual only
    start_date=datetime(2024, 1, 1),
    catchup=False,
    tags=["test", "docker"],
) as test_dag:
    
    test_docker = DockerOperator(
        task_id="test_docker",
        image="python:3.11-slim",
        command='python -c "print(\'Hello from Docker container!\')"',
        docker_url="unix://var/run/docker.sock",
        network_mode=DOCKER_NETWORK,
        auto_remove=True,
    )

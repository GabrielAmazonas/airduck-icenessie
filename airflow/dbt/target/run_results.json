{"metadata": {"dbt_schema_version": "https://schemas.getdbt.com/dbt/run-results/v5.json", "dbt_version": "1.7.4", "generated_at": "2026-01-24T01:00:24.387023Z", "invocation_id": "8f6b9f99-ab53-469b-a0a2-4b572d629ad3", "env": {}}, "results": [{"status": "success", "timing": [{"name": "compile", "started_at": "2026-01-24T01:00:23.558262Z", "completed_at": "2026-01-24T01:00:23.562590Z"}, {"name": "execute", "started_at": "2026-01-24T01:00:23.563434Z", "completed_at": "2026-01-24T01:00:24.381186Z"}], "thread_id": "Thread-1 (worker)", "execution_time": 0.8254270553588867, "adapter_response": {"_message": "SUCCESS", "rows_affected": 0, "query": "/* {\"app\": \"dbt\", \"dbt_version\": \"1.7.4\", \"profile_name\": \"trino_iceberg\", \"target_name\": \"dev\", \"node_id\": \"model.airduck_transforms.gold_documents\"} */\n\n  \n    \n\n    create table \"iceberg\".\"silver_gold\".\"gold_documents__dbt_tmp\"\n      \n      \n    as (\n      \n\n/*\n * Gold Layer: Vector-Ready Documents\n * ===================================\n * - Chunks long documents into smaller pieces\n * - Computes quality scores for filtering\n * - Ready for embedding generation by DuckDB/FastAPI\n */\n\nWITH silver AS (\n    SELECT * FROM \"iceberg\".\"silver_silver\".\"silver_documents\"\n),\n\n-- Generate chunk indices for documents\nchunk_indices AS (\n    SELECT \n        id as original_id,\n        content,\n        source,\n        content_hash,\n        metadata,\n        processed_at,\n        CAST(CEIL(LENGTH(content) / 1000.0) AS INTEGER) as num_chunks\n    FROM silver\n    WHERE LENGTH(content) > 0\n),\n\n-- Expand into chunks (simplified chunking - production should use overlap)\nchunked AS (\n    SELECT \n        original_id || '-' || CAST(chunk_idx AS VARCHAR) as id,\n        CASE \n            WHEN num_chunks = 1 THEN content\n            ELSE SUBSTR(\n                content, \n                (chunk_idx - 1) * 1000 + 1, \n                1000\n            )\n        END as content,\n        chunk_idx as chunk_index,\n        num_chunks as total_chunks,\n        metadata,\n        source,\n        content_hash as original_content_hash,\n        processed_at\n    FROM chunk_indices\n    CROSS JOIN UNNEST(SEQUENCE(1, GREATEST(1, num_chunks))) AS t(chunk_idx)\n),\n\n-- Compute quality scores\nscored AS (\n    SELECT \n        *,\n        -- Quality score based on content characteristics (0.0 - 1.0)\n        (\n            -- Length score (0.0 - 0.3)\n            CASE \n                WHEN LENGTH(content) > 200 THEN 0.3\n                WHEN LENGTH(content) > 100 THEN 0.2\n                ELSE 0.1 \n            END +\n            -- Has proper case (0.0 - 0.2)\n            CASE WHEN REGEXP_LIKE(content, '[A-Z]') THEN 0.2 ELSE 0.0 END +\n            -- Word count (0.0 - 0.3)\n            CASE \n                WHEN CARDINALITY(SPLIT(content, ' ')) > 20 THEN 0.3\n                WHEN CARDINALITY(SPLIT(content, ' ')) > 10 THEN 0.2\n                ELSE 0.1 \n            END +\n            -- Not blank (0.0 - 0.2)\n            CASE WHEN NOT REGEXP_LIKE(content, '^\\s*$') THEN 0.2 ELSE 0.0 END\n        ) as quality_score\n    FROM chunked\n    WHERE LENGTH(TRIM(content)) > 0\n)\n\nSELECT \n    id,\n    content,\n    chunk_index,\n    total_chunks,\n    metadata,\n    source,\n    original_content_hash,\n    -- Embedding placeholder (computed by DuckDB/FastAPI later)\n    CAST(NULL AS ARRAY(REAL)) as embedding,\n    quality_score,\n    CURRENT_TIMESTAMP as ready_at\nFROM scored\nWHERE quality_score >= 0.5\n    )", "query_id": "20260124_010023_00023_4yxt4"}, "message": "SUCCESS", "failures": null, "unique_id": "model.airduck_transforms.gold_documents", "compiled": true, "compiled_code": "\n\n/*\n * Gold Layer: Vector-Ready Documents\n * ===================================\n * - Chunks long documents into smaller pieces\n * - Computes quality scores for filtering\n * - Ready for embedding generation by DuckDB/FastAPI\n */\n\nWITH silver AS (\n    SELECT * FROM \"iceberg\".\"silver_silver\".\"silver_documents\"\n),\n\n-- Generate chunk indices for documents\nchunk_indices AS (\n    SELECT \n        id as original_id,\n        content,\n        source,\n        content_hash,\n        metadata,\n        processed_at,\n        CAST(CEIL(LENGTH(content) / 1000.0) AS INTEGER) as num_chunks\n    FROM silver\n    WHERE LENGTH(content) > 0\n),\n\n-- Expand into chunks (simplified chunking - production should use overlap)\nchunked AS (\n    SELECT \n        original_id || '-' || CAST(chunk_idx AS VARCHAR) as id,\n        CASE \n            WHEN num_chunks = 1 THEN content\n            ELSE SUBSTR(\n                content, \n                (chunk_idx - 1) * 1000 + 1, \n                1000\n            )\n        END as content,\n        chunk_idx as chunk_index,\n        num_chunks as total_chunks,\n        metadata,\n        source,\n        content_hash as original_content_hash,\n        processed_at\n    FROM chunk_indices\n    CROSS JOIN UNNEST(SEQUENCE(1, GREATEST(1, num_chunks))) AS t(chunk_idx)\n),\n\n-- Compute quality scores\nscored AS (\n    SELECT \n        *,\n        -- Quality score based on content characteristics (0.0 - 1.0)\n        (\n            -- Length score (0.0 - 0.3)\n            CASE \n                WHEN LENGTH(content) > 200 THEN 0.3\n                WHEN LENGTH(content) > 100 THEN 0.2\n                ELSE 0.1 \n            END +\n            -- Has proper case (0.0 - 0.2)\n            CASE WHEN REGEXP_LIKE(content, '[A-Z]') THEN 0.2 ELSE 0.0 END +\n            -- Word count (0.0 - 0.3)\n            CASE \n                WHEN CARDINALITY(SPLIT(content, ' ')) > 20 THEN 0.3\n                WHEN CARDINALITY(SPLIT(content, ' ')) > 10 THEN 0.2\n                ELSE 0.1 \n            END +\n            -- Not blank (0.0 - 0.2)\n            CASE WHEN NOT REGEXP_LIKE(content, '^\\s*$') THEN 0.2 ELSE 0.0 END\n        ) as quality_score\n    FROM chunked\n    WHERE LENGTH(TRIM(content)) > 0\n)\n\nSELECT \n    id,\n    content,\n    chunk_index,\n    total_chunks,\n    metadata,\n    source,\n    original_content_hash,\n    -- Embedding placeholder (computed by DuckDB/FastAPI later)\n    CAST(NULL AS ARRAY(REAL)) as embedding,\n    quality_score,\n    CURRENT_TIMESTAMP as ready_at\nFROM scored\nWHERE quality_score >= 0.5", "relation_name": "\"iceberg\".\"silver_gold\".\"gold_documents\""}], "elapsed_time": 1.1097567081451416, "args": {"print": true, "introspect": true, "partial_parse": true, "invocation_command": "dbt run --select gold --profiles-dir /opt/airflow/dbt --target dev --full-refresh", "log_file_max_bytes": 10485760, "send_anonymous_usage_stats": true, "log_format": "default", "project_dir": "/opt/airflow/dbt", "exclude": [], "use_colors_file": true, "log_level_file": "debug", "defer": false, "log_path": "/opt/airflow/dbt/logs", "use_colors": true, "profiles_dir": "/opt/airflow/dbt", "version_check": true, "log_format_file": "debug", "enable_legacy_logger": false, "partial_parse_file_diff": true, "printer_width": 80, "strict_mode": false, "quiet": false, "target": "dev", "vars": {}, "favor_state": false, "select": ["gold"], "full_refresh": true, "log_level": "info", "macro_debugging": false, "which": "run", "indirect_selection": "eager", "write_json": true, "populate_cache": true, "static_parser": true, "warn_error_options": {"include": [], "exclude": []}, "show_resource_report": false, "cache_selected_only": false}}
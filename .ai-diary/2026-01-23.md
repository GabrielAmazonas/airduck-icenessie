# AI Development Diary - January 23, 2026

## Session Summary

### Issues Addressed

#### 1. Build Index Not Picking Up Latest UI Documents

**Problem**: Documents added via the UI (`/index` endpoint) were not being included when the `daily_reindex` DAG rebuilt the vector index.

**Root Cause Analysis**:
- UI documents are written directly to the current DuckDB index file via FastAPI
- The `build_and_swap_index` function creates a NEW DuckDB file and reads only from:
  1. Iceberg Gold layer (if `USE_ICEBERG_SOURCE=true`)
  2. S3 parquet files (`s3://rag-data/documents/**/*.parquet`)
- There was no mechanism to sync documents from the current index to these sources
- Result: UI-added documents were lost on every reindex

**Solution Implemented**:
- Added new function `_ingest_from_current_index()` in `reindex_dag.py`
- This function reads all documents from the current active DuckDB index before building a new one
- Uses `INSERT OR REPLACE` to handle duplicates gracefully
- Modified `build_and_swap_index()` to always call this function first, then layer in Iceberg/S3 data

**Files Changed**:
- `airflow/dags/reindex_dag.py`: Added `_ingest_from_current_index()` function and integrated it into the build flow

**Verification**:
- Triggered manual DAG run
- Logs confirm: "Found 5 documents in current index, ingesting..."
- Documents with existing embeddings are preserved

---

#### 2. Backup to S3 Task Failing

**Problem**: The `backup_to_s3` task in `daily_reindex` DAG was failing with error: `Unexpected response while initializing S3 multipart upload`

**Root Cause Analysis**:
- The MinIO buckets (`rag-data` and `iceberg-warehouse`) did not exist
- The `minio-init` container had run previously with older containers from `duckdb-aws-vector-store` project
- When the stack was restarted with new containers, the bucket initialization ran but the buckets were lost due to volume mismatch
- DuckDB's S3 COPY command failed because the target bucket didn't exist

**Solution Implemented**:
- Manually created the missing buckets using MinIO client:
  ```bash
  mc mb myminio/rag-data
  mc mb myminio/iceberg-warehouse
  ```

**Verification**:
- Triggered manual DAG run
- Logs confirm: "Backup complete to s3://rag-data/backup/documents/year=2026/month=1/day=23/"
- Verified data exists: `data_0.parquet` (8.9KiB) in the backup path

---

### Tests Added

Created unit tests in `airflow/tests/test_reindex_dag.py`:
- `TestIngestFromCurrentIndex`: Tests document preservation during ingestion
- `TestDocumentDeduplication`: Tests INSERT OR REPLACE behavior for duplicates

---

### Commands Used

```bash
# Check Docker container conflicts
docker ps -a --filter "name=polary_"
docker stop polary_trino polary_polaris && docker rm $(docker ps -a --filter "name=polary_" -q)

# Create MinIO buckets
docker run --rm --network polary_platform --entrypoint sh minio/mc -c \
  "mc alias set myminio http://minio:9000 minioadmin minioadmin && \
   mc mb --ignore-existing myminio/rag-data && \
   mc mb --ignore-existing myminio/iceberg-warehouse"

# Trigger DAG for testing
docker exec polary_airflow_scheduler airflow dags trigger daily_reindex

# Verify backup data
docker run --rm --network polary_platform --entrypoint sh minio/mc -c \
  "mc alias set myminio http://minio:9000 minioadmin minioadmin && \
   mc ls --recursive myminio/rag-data/backup/"
```

---

### Architecture Diagram (Updated)

```
UI Upload Flow (NOW PRESERVED):
┌─────────────────────────────────────────────────────────────┐
│ frontend → POST /index → app/main.py                        │
│   → DuckDB index file (documents table)                     │
│   → Next build_and_swap_index reads from here ✅            │
└─────────────────────────────────────────────────────────────┘

Build Index Flow (UPDATED):
┌─────────────────────────────────────────────────────────────┐
│ reindex_dag.py:build_and_swap_index()                       │
│   1. _ingest_from_current_index() ← NEW! Preserves UI docs  │
│   2. _ingest_from_iceberg_gold() or _ingest_from_s3_parquet │
│   3. _generate_missing_embeddings()                          │
│   4. Build HNSW index                                        │
│   5. Atomic pointer swap                                     │
└─────────────────────────────────────────────────────────────┘
```

---

---

#### 3. dbt_manual_transforms DAG Failing (Polaris OAuth Scope Issue)

**Problem**: The `dbt_manual_transforms` DAG was failing at the `create_bronze_table` task with authentication errors when connecting to Apache Polaris.

**Root Cause Analysis**:
- Trino's Iceberg REST catalog connector sends `scope=catalog` in OAuth2 token requests
- Apache Polaris requires `scope=PRINCIPAL_ROLE:ALL` format
- This is a fundamental compatibility issue between Trino and Polaris
- Polaris logs showed: `Invalid scope provided. scopes=catalogscopes=catalog`
- No configuration option exists in either Trino or Polaris to change this behavior
- All Polaris Docker images (1.0.0+) are Quarkus-based and have this same issue

**Solution Implemented**:
- Replaced Apache Polaris with **Project Nessie** as the Iceberg catalog
- Nessie supports optional authentication (disabled for POC)
- Nessie works seamlessly with Trino's Iceberg connector

**Files Changed**:
- `docker-compose.yml`: Replaced `polaris` service with `nessie` service
- `trino/catalog/iceberg.properties`: Changed from REST catalog to Nessie catalog type
- `airflow/dbt/models/silver/silver_documents.sql`: Fixed type mismatch (VARCHAR to MAP cast)

**Configuration Changes**:

```yaml
# docker-compose.yml - Nessie service
nessie:
  image: ghcr.io/projectnessie/nessie:0.76.0
  container_name: polary_nessie
  ports:
    - "19120:19120"
  environment:
    NESSIE_SERVER_AUTHENTICATION_ENABLED: "false"
    NESSIE_VERSION_STORE_TYPE: IN_MEMORY
```

```properties
# trino/catalog/iceberg.properties
iceberg.catalog.type=nessie
iceberg.nessie-catalog.uri=http://nessie:19120/api/v1
iceberg.nessie-catalog.ref=main
iceberg.nessie-catalog.default-warehouse-dir=s3://iceberg-warehouse/
```

**Additional Fix** - dbt model type mismatch:
- `silver_documents.sql`: Changed `TRY_CAST(raw_metadata AS MAP(...))` to just `raw_metadata as metadata`
- Trino cannot directly cast VARCHAR to MAP; JSON parsing should be done downstream

**Verification**:
- All 12 tasks in `dbt_manual_transforms` DAG now succeed
- Iceberg tables created successfully in Nessie
- dbt models (silver, gold) run without errors

---

---

#### 4. Architecture Documentation

Created comprehensive Staff/Principal engineer level architecture documentation in `ARCHITECTURE.md` covering:

- Executive summary and value proposition
- Component deep-dive (DuckDB, Iceberg, Trino, dbt, Airflow)
- Medallion architecture (Bronze → Silver → Gold)
- Pointer-based atomic swap pattern for zero-downtime updates
- Cost analysis vs. managed vector database services
- Trade-offs and production readiness checklist

---

---

#### 5. Polaris Cleanup

**Problem**: After migrating to Nessie, the project still contained Polaris-related files and references that were no longer needed.

**Files Deleted**:
- `polaris/etc/polaris-server.yml` - Polaris server configuration
- `polaris/` directory - Entire Polaris configuration folder
- `trino/scripts/generate-token.sh` - OAuth token generation script (was for Polaris)
- `trino/scripts/entrypoint.sh` - Custom entrypoint for token generation
- `trino/scripts/` directory - No longer needed
- `trino/Dockerfile` - Custom Trino build for Polaris integration

**Files Updated**:
- `docker-compose.yml`: Updated header comment (Polaris → Nessie), cleaned service description
- `ARCHITECTURE.md`: Replaced Polaris comparison table with Nessie benefits section
- `NEXT_STEPS.md`: Complete rewrite focused on Nessie-based architecture and production roadmap

---

---

#### 6. Project Rename: trilarisduck-air → airduck-icenessie

**Changes Made**:
- Renamed root folder from `trilarisduck-air` to `airduck-icenessie`
- Updated project name in `ARCHITECTURE.md` and `NEXT_STEPS.md`
- Updated Docker container/network/volume names: `polary_*` → `airduck_*`
- Updated dbt project name: `polary_transforms` → `airduck_transforms`
- Updated Trino node ID: `polary-trino-node` → `airduck-trino-node`

---

### Status

| Task | Status |
|------|--------|
| Fix build_index to preserve UI documents | ✅ Complete |
| Fix S3 backup (bucket creation) | ✅ Complete |
| Add unit tests | ✅ Complete |
| Fix dbt_manual_transforms (Polaris → Nessie) | ✅ Complete |
| Create ARCHITECTURE.md documentation | ✅ Complete |
| Remove Polaris artifacts and references | ✅ Complete |
| Rename project to airduck-icenessie | ✅ Complete |
| Update .ai-diary | ✅ Complete |

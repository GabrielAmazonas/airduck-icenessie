# =============================================================================
# Airduck-IceNessie - Docker Compose Configuration
# =============================================================================
# Services:
#   - MinIO (S3-compatible storage)
#   - DuckDB Server (FlightSQL vector database)
#   - FastAPI (Search API)
#   - Frontend (Next.js UI)
#   - Airflow (Orchestration + dbt)
#   - Nessie (Iceberg Catalog)
#   - Trino (Query Engine)
#   - PostgreSQL (Airflow metadata)
# =============================================================================

# -----------------------------------------------------------------------------
# Shared Airflow Configuration
# -----------------------------------------------------------------------------
x-airflow-common: &airflow-common
  build: ./airflow
  environment: &airflow-common-env
    AIRFLOW__CORE__EXECUTOR: LocalExecutor
    AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow
    AIRFLOW__CORE__FERNET_KEY: ''
    AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true'
    AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
    AIRFLOW__WEBSERVER__SECRET_KEY: 'rag-stack-secret-key-change-in-production'
    # S3/MinIO Configuration
    S3_ENDPOINT: http://minio:9000
    S3_ACCESS_KEY: minioadmin
    S3_SECRET_KEY: minioadmin
    AWS_ACCESS_KEY_ID: minioadmin
    AWS_SECRET_ACCESS_KEY: minioadmin
    # Storage paths
    EFS_PATH: /mnt/efs
    # dbt Configuration
    DBT_PROFILES_DIR: /opt/airflow/dbt
    DBT_PROJECT_DIR: /opt/airflow/dbt
    # Trino connection for dbt
    TRINO_HOST: trino
    TRINO_PORT: 8080
    TRINO_USER: airflow
    TRINO_CATALOG: iceberg
    # DuckDB Server
    DUCKDB_SERVER: duckdb-server:8815
    # Iceberg source toggle (set to 'true' to read from Iceberg Gold layer)
    USE_ICEBERG_SOURCE: 'false'
  volumes:
    - ./airflow/dags:/opt/airflow/dags
    - ./airflow/dbt:/opt/airflow/dbt
    - efs-data:/mnt/efs
  networks:
    - platform_net

# -----------------------------------------------------------------------------
# Networks
# -----------------------------------------------------------------------------
networks:
  platform_net:
    driver: bridge
    name: airduck_platform

# -----------------------------------------------------------------------------
# Volumes
# -----------------------------------------------------------------------------
volumes:
  efs-data:
    name: airduck_efs
  postgres-data:
    name: airduck_postgres

# =============================================================================
# SERVICES
# =============================================================================
services:

  # ---------------------------------------------------------------------------
  # 1. OBJECT STORAGE (MinIO - S3 Compatible)
  # ---------------------------------------------------------------------------
  minio:
    image: minio/minio:RELEASE.2024-01-16T16-07-38Z
    container_name: airduck_minio
    ports:
      - "9000:9000"   # S3 API
      - "9001:9001"   # Console UI
    environment:
      MINIO_ROOT_USER: minioadmin
      MINIO_ROOT_PASSWORD: minioadmin
    command: server /data --console-address ":9001"
    volumes:
      - ./minio-data:/data
    networks:
      - platform_net
    restart: unless-stopped

  # MinIO bucket initialization
  minio-init:
    image: minio/mc:RELEASE.2024-01-16T16-06-34Z
    container_name: airduck_minio_init
    depends_on:
      minio:
        condition: service_started
    entrypoint: >
      /bin/sh -c "
      until mc alias set myminio http://minio:9000 minioadmin minioadmin; do
        echo '...waiting for minio...';
        sleep 1;
      done;
      
      # Create buckets
      mc mb --ignore-existing myminio/rag-data;
      mc mb --ignore-existing myminio/iceberg-warehouse;
      
      # Set bucket policies (optional: make public for testing)
      mc anonymous set download myminio/rag-data;
      
      echo 'Buckets created: rag-data, iceberg-warehouse';
      "
    networks:
      - platform_net

  # Iceberg schema initialization (rebuilds after Nessie restart)
  iceberg-init:
    image: trinodb/trino:439
    container_name: airduck_iceberg_init
    depends_on:
      trino:
        condition: service_healthy
      minio-init:
        condition: service_completed_successfully
    entrypoint: >
      /bin/sh -c "
      echo 'Waiting for Trino to be fully ready...';
      sleep 5;
      
      echo 'Creating Iceberg schemas...';
      
      trino --server http://trino:8080 --execute \"CREATE SCHEMA IF NOT EXISTS iceberg.bronze\" || true;
      trino --server http://trino:8080 --execute \"CREATE SCHEMA IF NOT EXISTS iceberg.silver\" || true;
      trino --server http://trino:8080 --execute \"CREATE SCHEMA IF NOT EXISTS iceberg.gold\" || true;
      
      echo 'Iceberg schemas created: bronze, silver, gold';
      echo 'Note: Run dbt_seed_bronze_data DAG to create tables with data';
      "
    networks:
      - platform_net

  # ---------------------------------------------------------------------------
  # 2. DUCKDB SERVER (FlightSQL Protocol)
  # ---------------------------------------------------------------------------
  # NOTE: Commented out - Currently using embedded DuckDB in FastAPI instead.
  #       The duckdb-server holds a lock on the EFS database file which conflicts
  #       with FastAPI's embedded DuckDB. Uncomment if you want to use FlightSQL
  #       client-server architecture (requires updating FastAPI to use ADBC client).
  # ---------------------------------------------------------------------------
  # duckdb-server:
  #   build:
  #     context: ./duckdb-server
  #     dockerfile: Dockerfile
  #   container_name: airduck_duckdb
  #   ports:
  #     - "8815:8815"   # FlightSQL (gRPC)
  #     - "8816:8816"   # Health check HTTP
  #   environment:
  #     - EFS_PATH=/mnt/efs
  #     - S3_ENDPOINT=minio:9000
  #     - AWS_ACCESS_KEY_ID=minioadmin
  #     - AWS_SECRET_ACCESS_KEY=minioadmin
  #     - FLIGHT_PORT=8815
  #     - HEALTH_PORT=8816
  #   volumes:
  #     - efs-data:/mnt/efs
  #   depends_on:
  #     minio:
  #       condition: service_started
  #   healthcheck:
  #     test: ["CMD", "curl", "-f", "http://localhost:8816/health"]
  #     interval: 30s
  #     timeout: 10s
  #     retries: 5
  #     start_period: 60s
  #   networks:
  #     - platform_net

  # ---------------------------------------------------------------------------
  # 3. SEARCH API (FastAPI)
  # ---------------------------------------------------------------------------
  fastapi:
    build: ./app
    container_name: airduck_search_api
    ports:
      - "8000:8000"
    environment:
      - EFS_PATH=/mnt/efs
      - S3_ENDPOINT=http://minio:9000
      - AWS_ACCESS_KEY_ID=minioadmin
      - AWS_SECRET_ACCESS_KEY=minioadmin
      - AWS_REGION=us-east-1
      - DUCKDB_SERVER=duckdb-server:8815
    volumes:
      - efs-data:/mnt/efs
    depends_on:
      minio:
        condition: service_started
    networks:
      - platform_net

  # ---------------------------------------------------------------------------
  # 4. FRONTEND (Next.js)
  # ---------------------------------------------------------------------------
  frontend:
    build: ./frontend
    container_name: airduck_frontend
    ports:
      - "3000:3000"
    environment:
      - NEXT_PUBLIC_API_URL=http://localhost:8000
    depends_on:
      - fastapi
    networks:
      - platform_net

  # ---------------------------------------------------------------------------
  # 5. ICEBERG CATALOG (Project Nessie)
  # ---------------------------------------------------------------------------
  nessie:
    image: ghcr.io/projectnessie/nessie:0.76.0
    container_name: airduck_nessie
    ports:
      - "19120:19120"   # REST API
    environment:
      # No authentication for POC
      NESSIE_SERVER_AUTHENTICATION_ENABLED: "false"
      # Use in-memory storage for POC
      QUARKUS_PROFILE: prod
      NESSIE_VERSION_STORE_TYPE: IN_MEMORY
    depends_on:
      minio:
        condition: service_started
    networks:
      - platform_net
    restart: unless-stopped

  # ---------------------------------------------------------------------------
  # 6. QUERY ENGINE (Trino)
  # ---------------------------------------------------------------------------
  trino:
    image: trinodb/trino:439
    container_name: airduck_trino
    ports:
      - "8085:8080"   # Web UI & JDBC (using 8085 to avoid Airflow conflict)
    volumes:
      - ./trino/etc:/etc/trino
      - ./trino/catalog:/etc/trino/catalog
    environment:
      - JAVA_TOOL_OPTIONS=-Xmx4g
    depends_on:
      nessie:
        condition: service_started
      minio:
        condition: service_started
    networks:
      - platform_net
    restart: unless-stopped

  # ---------------------------------------------------------------------------
  # 7. METADATA DATABASE (PostgreSQL)
  # ---------------------------------------------------------------------------
  postgres:
    image: postgres:15.5-alpine
    container_name: airduck_postgres
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    volumes:
      - postgres-data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U airflow"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - platform_net

  # ---------------------------------------------------------------------------
  # 8. ORCHESTRATOR (Airflow) - Includes dbt
  # ---------------------------------------------------------------------------
  airflow-init:
    <<: *airflow-common
    container_name: airduck_airflow_init
    user: root
    entrypoint: /bin/bash
    command:
      - -c
      - |
        # Fix EFS permissions for airflow user
        chown -R 50000:0 /mnt/efs
        chmod -R 775 /mnt/efs
        
        # Fix dbt directory permissions
        chown -R 50000:0 /opt/airflow/dbt || true
        chmod -R 775 /opt/airflow/dbt || true
        
        # Run as airflow user for DB init
        su airflow -c "airflow db init"
        su airflow -c "airflow users create \
          --username airflow \
          --password airflow \
          --firstname Admin \
          --lastname User \
          --role Admin \
          --email admin@example.com || true"
        
        # Install dbt packages if packages.yml exists
        if [ -f /opt/airflow/dbt/packages.yml ]; then
          su airflow -c "cd /opt/airflow/dbt && dbt deps" || true
        fi
        
        echo "Airflow initialized successfully"
    depends_on:
      postgres:
        condition: service_healthy
    restart: on-failure

  airflow-webserver:
    <<: *airflow-common
    container_name: airduck_airflow_webserver
    command: webserver
    ports:
      - "8080:8080"   # Airflow UI
    depends_on:
      airflow-init:
        condition: service_completed_successfully
      iceberg-init:
        condition: service_completed_successfully
    restart: on-failure

  airflow-scheduler:
    <<: *airflow-common
    container_name: airduck_airflow_scheduler
    command: scheduler
    depends_on:
      airflow-init:
        condition: service_completed_successfully
      iceberg-init:
        condition: service_completed_successfully
    restart: on-failure
